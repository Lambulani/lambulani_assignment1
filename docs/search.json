[
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Data Science for Industry: Assignment 1",
    "section": "",
    "text": "The State of the Nation Address (SONA) is an annual event in South Africa where the President addresses the nation, providing a comprehensive overview of the country’s political, social, and economic landscape(“State of the Nation Address,” n.d.).SONA speeches are pivotal moments that set the direction for policy and reform, and they also serve as historical records of each presidency. In this context, this research endeavors to construct predictive models to determine which South African president delivered a particular sentence from a SONA speech.\nThe ability to attribute sentences to specific presidents within SONA speeches carries substantial relevance for various stakeholders, including political analysts, historians, and policymakers. Such predictive models could streamline the process of content analysis and enable more efficient tracking of policy changes, political discourse, and shifts in leadership emphasis over time. Additionally, these models offer practical applications in automated speech transcription and archiving."
  },
  {
    "objectID": "Assignment1.html#literature-review",
    "href": "Assignment1.html#literature-review",
    "title": "Data Science for Industry: Assignment 1",
    "section": "Literature Review",
    "text": "Literature Review\nRecently, (Nivash et al. 2022) studied several presidential speeches from 1970 to 2019, using classificaition models such as Naive Bayes to predict sentiment based off the speeches.Also (Ficcadenti, Cerqueti, and Ausloos 2019) combines text mining and rank-size analysis to explore the structures and word distributions within US presidential speeches. Authors (Finity, Garg, and McGaw 2021) implemeneted Natural Language Processing techniques to study things like sentiment and topic variance on speeches made by a few candidates of the 2020 US presidential elections. In (Jin and Mihalcea 2022) suggests NLP methods, including text classification and topic modelling, investigate policy making decisions and their effects on the public. A paper by (Schmidt and Wiegand 2017) highlights the use of Support Vector Machinees(SVM) in detecting hate speech. In (Alshalan et al. 2020) used Convolutional Neural Networks(CNNs) to classificaiton of hate speech in tweets during the COVID-19 pandemic. In their paper, (Boschee, Natarajan, and Weischedel 2012) showed that using Natural Language Processing methods for prediction forecasting of events yielded better accuracy than other methods employed."
  },
  {
    "objectID": "Assignment1.html#data-and-methodology",
    "href": "Assignment1.html#data-and-methodology",
    "title": "Data Science for Industry: Assignment 1",
    "section": "Data and Methodology",
    "text": "Data and Methodology\n\nData Preprocessing\nThe full text of State of the Nation Address (SONA) speeches, from 1994 to 2023, was collected from the official South African Governement website(“State of the Nation Address,” n.d.) Data preprocessing constitutes a critical step in text analysis, with the primary goal of converting unprocessed textual data into a well-structured format conducive to in-depth analysis of the speeches. In this context, we opted for the ‘tidytext’ package within the R programming language.\nThe initial data preprocessing involved identifying the speech date and president, followed by the removal of special characters, digits, and punctuation marks, as these elements can introduce noise and hinder the analysis, with the exclusion simplifying and focusing the text content. The following step involved structuring the data to prepare it for use to train the machine learning models and also to make predictions from these models. Tolkenizaiton is implemented on the data to extract meaningful information. In this paper, each speech is tolkenized by sentence and each sentence is given a unique identity number. This allows us to identify each sentence by its unique number and to which president the sentence was said by. All the characters in the text are converted to lower case. This ensures that the analysis is not case sensitive. Stop words, commonly occurring words that hold not much meaning, are removed: shifting the focus to content-carrying words, enhancing the identification of significant themes and patterns in the SONA speeches. The data is structured in such a way that it can be employed in machine learning models. Since the presidents did not have the same number of opportunities to give the SONA, the data is imbalanced, having more sentences belonging to the Presidents were in office for longer. To deal with this, when taking the 80/20 split between the training and test sets, each set was ensured to be balanced among the different presidents. This resulted to having a slightly smaller data set for training the model. The different ways to structure the input data are discussed below.\n\nBag of words\nBoW is a text representation method that transforms a document or piece of text into a structured format suitable for computational analysis. The core idea behind BoW is to create a “vocabulary” or a set of unique words (or tokens) found in a given corpus of text. Each document in the corpus is then represented as a vector, where the elements of the vector correspond to the frequency or presence of each word in the vocabulary. In this format, the each sentence can be represented in a way that the machine learning models in the classification task for this paper\n\n\nWord Embeddings\nWord embeddings are essentially numerical vector representations of words, learned through extensive neural network training on vast text corpora. They map words to points in a continuous vector space, where words with similar meanings or usage patterns are positioned closer together. The key advantage of embeddings is their ability to preserve the semantic relationships between words. Embeddings enable neural networks to understand and process text at a more granular level, as opposed to the BoW approach, which simply counts word occurrences\n\n\nTerm Frequency-Inverse Document Frequency (TF-IDF)\nTF_IDF is a statistical structuring of data method that evaluates the importance of a term within a document relative to its frequency across a collection of documents. The TF-IDF model operates on the principle that words that occur frequently in one document but less frequently in others are indicative of that document’s subject matter. Tf-idf features often give better accuracy in predictive modelling than using word frequencies.\nTerm Frequency (TF): \\[\\begin{equation}\n\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n\\end{equation}\\]\nInverse Document Frequency (IDF): \\[\\begin{equation}\n\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{Total number of documents in the corpus } |D|}{\\text{Number of documents containing term } t + 1}\\right)\n\\end{equation}\\]\nTF-IDF: \\[\\begin{equation}\n\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n\\end{equation}\\]\n\n\n\nModels\nThe machine learning models used in the classification problem were Feed-Forward Neural Netowrk, Convolutional Neural Network, Random Forest and Gradient Boosts.\n\nFeed forward Neural Network\nA feedforward neural network, often referred to as a multilayer perceptron (MLP), is a fundamental artificial neural network architecture used for various machine learning and pattern recognition tasks. Each neuron in a layer receives input from neurons in the previous layer, applies a weighted sum of these inputs, and then passes the result through an activation function. The activation functions used in the hidden layers was Rectified Linear Units (ReLU) and in the output layer was the softmax which is appropriate for classification tasks. Training a feedforward neural network involves adjusting the weights assigned to each connection between neurons to minimize the difference between the predicted output and the actual target values using Backpropagation and optimization algorithms.\n\\[\\begin{align*}\nw_{ij}^{(l)} &\\leftarrow w_{ij}^{(l)} - \\alpha \\frac{\\partial C}{\\partial w_{ij}^{(l)}} \\\\\nw_{ij}^{(l)} &\\text{ is the weight being optimized at layer }l \\\\\n\\alpha &\\text{ is the learning rate} \\\\\n\\frac{\\partial C}{\\partial w_{ij}^{(l)}} &\\text{ is the partial derivative of the Cost function being optimized}\n\\end{align*}\\]\nA learning rate,, of 0.5 was used and Rectified Linear Units (ReLU) activation in the hidden layer. The softmax activation function was used in the output layer.\n\n\nConvolutional Neural Networks (CNNs)\nCNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The key innovation in CNNs is the use of convolutional layers, which apply a set of learnable filters (kernels) to input data, extracting spatial features and patterns from the images.Pooling layers reduce the spatial dimensions of the feature maps, effectively down-sampling the data and retaining important information. Fully connected layers process the extracted features to make predictions or classification\nThe convolution operation applies a filter (kernel) to an input image to produce a feature map. In this equation, I represents the input image, K is the filter (kernel), and denotes the convolution operation.\n\\[\\begin{align*}\n(I * K)(x, y) = \\sum_{i} \\sum_{j} I(x - i, y - j) * K(i, j)\\\\\n\\text{Max-Pooling}(I)(x, y) = \\max_{i, j} I(sx + i, sy + j)\n\\end{align*}\\]\nIn this paper, the learning rate in the dropout layer was tuned. A ReLU activation function was used in the fully connected hidden layer,while a softmax activation was used in the output layer.\n\n\nRandom Forests\nRandom Forest works by constructing a forest of decision trees, each trained on a random subset of the data and a random subset of the features. This randomness ensures diversity among the individual trees, making the ensemble less prone to overfitting and more resilient to noise in the data.\n\\[\\begin{align*}\n&\\text{Splitting Criteria:} \\quad J(D, f) = \\sum_{d \\in D} \\left( p_d \\cdot H(y_{d}) - p_{l} \\cdot H(y_{l}) - p_{r} \\cdot H(y_{r}) \\right) \\\\\n&\\text{where} \\quad H(y) \\text{ is the impurity measure for class } y \\\\\n&\\text{Decision Tree:} \\quad f(X) = \\sum_{t=1}^{T} I(x \\in R_t) \\cdot c_t\n\\end{align*}\\]\nIn training the model, this paper performed 5 fold cross-validation across a grid of the hyperparameter that controls the number of features randomly selected at each split when growing a tree.\n\n\nGradient Boosting Machine (GBMs)\nGBMs fall under the category of ensemble learning, where multiple weak learners (typically decision trees) are combined to form a strong predictive model. Through an iterative process of models,the algorithm used assigns higher weights to the data points that were misclassified or predicted with high error in the previous model.\n\\[\\begin{align*}\nL(\\theta) = \\sum_{i=1}^{n} l(y_i, F(x_i, \\theta)) + \\Omega(\\theta)\n\\end{align*}\\]\nHere, \\(l\\) is the loss function, \\(F(x_i, \\theta)\\) is the current model’s prediction, and \\(\\Omega(\\theta)\\) represents a regularization term to prevent overfitting.\n\\[\n\\theta_{t+1} = \\theta_t - \\nu \\nabla L(\\theta_t)\\\\F(x) = \\sum_{t=1}^{T} F_t(x)\n\\]\nThe final prediction, \\(F(x)\\), is the summation of predictions from each boosting iteration. In this paper, we perform a grid search over the learning rate,\\(\\nu\\),(0.01,0.0005, 0.0001) and the number of trees (100, 200, 500) to be included in each ensemble, \\(T\\). These hyper parameters are tuned and 5-fold cross validation is carried out with each iteration. The hyperparemters that produce the lowest cross validation error is used to fit the final model."
  },
  {
    "objectID": "Assignment1.html#results",
    "href": "Assignment1.html#results",
    "title": "Data Science for Industry: Assignment 1",
    "section": "Results",
    "text": "Results\nThe training set was used to train the different models and the test set was used to make predictions.\n\n\n\n\n \n  \n    Model \n    Test Accuracy \n  \n \n\n  \n    Feed Forward Neural Network (Bag of Words structure) \n    0.4326 \n  \n  \n    Feed Forward Neural Network (Embeded Words structure) \n    0.5203 \n  \n  \n    Convolutional Neural Nework (Embeded Words structure) \n    0.5094 \n  \n  \n    Random Forest \n    0.4519 \n  \n  \n    Gradient Boost Machine \n    0.4197 \n  \n\n\n\n\n\n?@tbl-accuracy shows the prediction accuracy of each of the models on the test set. It is seen that the standard feed forward neural network with embeded words structure performed the best on the test set having a test accuracy of around 0.5 which is slightly higher than the prediction accuracy of the CNN.\n\n\n\n\nTable 1:  Class representation of presidents \n\n  \n    clss \n    Class 0 \n    Class 1 \n    Class 2 \n    Class 3 \n    Class 4 \n    Class 5 \n  \n  \n    pres \n    De Klerk \n    Mandela \n    Mbeki \n    Mothlane \n    Zuma \n    Ramaphosa \n  \n\n\n\n\n\n\n\n\n\n\nTable 2:  Random Forest Prediction Performance Metrics \n \n  \n      \n    Sensitivity \n    Specificity \n    Precision \n    Recall \n    F1 \n  \n \n\n  \n    Class: 0 \n    0.0714 \n    0.9988 \n    0.3333 \n    0.0714 \n    0.1176 \n  \n  \n    Class: 1 \n    0.2740 \n    0.9039 \n    0.3598 \n    0.2740 \n    0.3111 \n  \n  \n    Class: 2 \n    0.4803 \n    0.7941 \n    0.4612 \n    0.4803 \n    0.4706 \n  \n  \n    Class: 3 \n    0.0000 \n    1.0000 \n    NA \n    0.0000 \n    NA \n  \n  \n    Class: 4 \n    0.4710 \n    0.8150 \n    0.4493 \n    0.4710 \n    0.4599 \n  \n  \n    Class: 5 \n    0.5720 \n    0.7541 \n    0.4810 \n    0.5720 \n    0.5226 \n  \n\n\n\n\n\n\n\n\n\n\nTable 3:  Gradient Boosts Prediction Performance Metrics \n \n  \n      \n    Sensitivity \n    Specificity \n    Precision \n    Recall \n    F1 \n  \n \n\n  \n    Class: 0 \n    0.0714 \n    0.9994 \n    0.5000 \n    0.0714 \n    0.1250 \n  \n  \n    Class: 1 \n    0.1317 \n    0.9670 \n    0.4405 \n    0.1317 \n    0.2027 \n  \n  \n    Class: 2 \n    0.4585 \n    0.7877 \n    0.4421 \n    0.4585 \n    0.4502 \n  \n  \n    Class: 3 \n    0.0000 \n    1.0000 \n    NA \n    0.0000 \n    NA \n  \n  \n    Class: 4 \n    0.3744 \n    0.8444 \n    0.4354 \n    0.3744 \n    0.4026 \n  \n  \n    Class: 5 \n    0.6440 \n    0.6098 \n    0.3967 \n    0.6440 \n    0.4910 \n  \n\n\n\n\n\n\nTable 1, Table 2, and Table 3 help us to analyze the performance of the predictions for each of the presidents in when random forests and GMB were implemented. It can be seen by the F1-score in both models perform bes in terms of classifying instances where the sentences belonging and not belonging to President Ramaphosa. Notably, in both Table 2 and Table 3 the sensitivity for predicting sentences made by De-Klerk and Mothlane were very low while specficity was close to 1 for both of them in the two methods. This shows that the models performed poorly at classifying sentences that belonged to the two presidents."
  },
  {
    "objectID": "Assignment1.html#conclusion",
    "href": "Assignment1.html#conclusion",
    "title": "Data Science for Industry: Assignment 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis paper studied the State of the Nation Addresses made by the previous six South African Presidents. Natural Language Processing (NLP) models were used to make predictions to classify sentences according to which president was the source of the sentence. The sentences were structured using the Bag of Words method. Feed Forward Neural networks performed best in terms of test accuracy, having a test accuracy of just around 0.5. Further studies could explore structuring the data as Term Frequency-Inverse Document Frequency format in order to obtain better predictions. This was not done in this paper due to computational limitations. Text mining and NLP can have very useful potential in speech classification,and reveal insights in the world of political communication.\n\n\nReferences\n\n\nAlshalan, Raghad, Hend Al-Khalifa, Duaa Alsaeed, Heyam Al-Baity, and Shahad Alshalan. 2020. “Detection of Hate Speech in Covid-19–Related Tweets in the Arab Region: Deep Learning and Topic Modeling Approach.” Journal of Medical Internet Research 22 (12): e22609.\n\n\nBoschee, Elizabeth, Premkumar Natarajan, and Ralph Weischedel. 2012. “Automatic Extraction of Events from Open Source Text for Predictive Forecasting.” In Handbook of Computational Approaches to Counterterrorism, 51–67. Springer.\n\n\nFiccadenti, Valerio, Roy Cerqueti, and Marcel Ausloos. 2019. “A Joint Text Mining-Rank Size Investigation of the Rhetoric Structures of the US Presidents’ Speeches.” Expert Systems with Applications 123: 127–42.\n\n\nFinity, Kevin, Ramit Garg, and Max McGaw. 2021. “A Text Analysis of the 2020 Us Presidential Election Campaign Speeches.” In 2021 Systems and Information Engineering Design Symposium (SIEDS), 1–6. IEEE.\n\n\nJin, Zhijing, and Rada Mihalcea. 2022. “Natural Language Processing for Policymaking.” In Handbook of Computational Social Science for Policy, 141–62. Springer International Publishing Cham.\n\n\nNivash, S, EN Ganesh, K Harisudha, and S Sreeram. 2022. “Extensive Analysis of Global Presidents’ Speeches Using Natural Language.” In Sentimental Analysis and Deep Learning: Proceedings of ICSADL 2021, 829–50. Springer.\n\n\nSchmidt, Anna, and Michael Wiegand. 2017. “A Survey on Hate Speech Detection Using Natural Language Processing.” In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, 1–10. Valencia, Spain: Association for Computational Linguistics. https://doi.org/10.18653/v1/W17-1101.\n\n\n“State of the Nation Address.” n.d. https://www.gov.za/state-nation-address."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bndlev001Assignment1",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]