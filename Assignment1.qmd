---
title: "Data Science for Industry: Assignment 1"
author: "Levy Banda"
student_number: "BNDLEV001"
editor: visual
embed-resources: true
bibliography: reference_assignment1.bib
format: html
execute: 
  warning: false
  echo: false
---

## Introduction

The State of the Nation Address (SONA) is an annual event in South Africa where the President addresses the nation, providing a comprehensive overview of the country's political, social, and economic landscape[@sagov].SONA speeches are pivotal moments that set the direction for policy and reform, and they also serve as historical records of each presidency. In this context, this research endeavors to construct predictive models to determine which South African president delivered a particular sentence from a SONA speech.

The ability to attribute sentences to specific presidents within SONA speeches carries substantial relevance for various stakeholders, including political analysts, historians, and policymakers. Such predictive models could streamline the process of content analysis and enable more efficient tracking of policy changes, political discourse, and shifts in leadership emphasis over time. Additionally, these models offer practical applications in automated speech transcription and archiving.

## Literature Review

Recently, [@nivash2022extensive] studied several presidential speeches from 1970 to 2019, using classificaition models such as Naive Bayes to predict sentiment based off the speeches.Also [@ficcadenti2019joint] combines text mining and rank-size analysis to explore the structures and word distributions within US presidential speeches. Authors [@finity2021text] implemeneted Natural Language Processing techniques to study things like sentiment and topic variance on speeches made by a few candidates of the 2020 US presidential elections. In [@jin2022natural] suggests NLP methods, including text classification and topic modelling, investigate policy making decisions and their effects on the public. A paper by [@schmidt-wiegand-2017-survey] highlights the use of Support Vector Machinees(SVM) in detecting hate speech. In [@alshalan2020detection] used Convolutional Neural Networks(CNNs) to classificaiton of hate speech in tweets during the COVID-19 pandemic. In their paper, [@boschee2012automatic] showed that using Natural Language Processing methods for prediction forecasting of events yielded better accuracy than other methods employed.

## Data and Methodology

### Data Preprocessing

The full text of State of the Nation Address (SONA) speeches, from 1994 to 2023, was collected from the official South African Governement website[@sagov] Data preprocessing constitutes a critical step in text analysis, with the primary goal of converting unprocessed textual data into a well-structured format conducive to in-depth analysis of the speeches. In this context, we opted for the 'tidytext' package within the R programming language.

The initial data preprocessing involved identifying the speech date and president, followed by the removal of special characters, digits, and punctuation marks, as these elements can introduce noise and hinder the analysis, with the exclusion simplifying and focusing the text content. The following step involved structuring the data to prepare it for use to train the machine learning models and also to make predictions from these models. Tolkenizaiton is implemented on the data to extract meaningful information. In this paper, each speech is tolkenized by sentence and each sentence is given a unique identity number. This allows us to identify each sentence by its unique number and to which president the sentence was said by. All the characters in the text are converted to lower case. This ensures that the analysis is not case sensitive. Stop words, commonly occurring words that hold not much meaning, are removed: shifting the focus to content-carrying words, enhancing the identification of significant themes and patterns in the SONA speeches. The data is structured in such a way that it can be employed in machine learning models. Since the presidents did not have the same number of opportunities to give the SONA, the data is imbalanced, having more sentences belonging to the Presidents were in office for longer. To deal with this, when taking the 80/20 split between the training and test sets, each set was ensured to be balanced among the different presidents. This resulted to having a slightly smaller data set for training the model. The different ways to structure the input data are discussed below.

```{r, include=FALSE}
# Clear global environment
rm(list=ls())
# Libraries we need
libs <- c('dplyr', 'ggplot2', 'lubridate', 'purrr', 'reshape2', 'stringr', 'text2vec', 'tidyr', 'tidytext', 'topicmodels', 'tm', 'wordcloud', "kableExtra", "keras", "tensorflow", "rpart", "randomForest", "gbm", "caret")
# Install missing libraries
installed_libs <- libs %in% rownames(installed.packages())
if (any(installed_libs == FALSE)) {
  install.packages(libs[!installed_libs], repos='http://cran.us.r-project.org')
}
# Load libraries
invisible(lapply(libs, library, character.only = TRUE))
```

```{r, include= FALSE}
unzip("sona-addresses-1994-2023.zip", exdir = "data")
```

```{r read_wrangle, include= FALSE}
# Get a list of all text files in the directory
text_files <- list.files(path = "data", pattern = ".txt")
# filenames <- purrr::flatten(text_files)
# Initialize an empty list to store the data
# speech_data <- list()
speech_data <- c()
i = 0
num_chars <- c(27050, 12786, 39019, 39524, 37489, 45247, 34674, 41225, 37552, 41719, 50544, 58284, 34590, 39232, 54635, 48643, 48641, 44907, 31101, 47157, 26384, 33281, 33376, 36006, 29403, 36233, 32860, 32464, 35981, 33290, 42112, 56960, 47910, 43352, 52972, 60000)
# Loop through the list of text files and read them into R
for (file in text_files) {
  i = i + 1
  # speech <- readLines(file, warn = FALSE)
  file_handle <- file(paste("data/", file, sep = ""), "r")
  speech <- readChar(file_handle, nchars = num_chars[i])
  # speech_data[[file]] <- speech
  speech_data[i] <- speech
  close(file_handle)
}

sona <- data.frame(filename = text_files, speech = speech_data, stringsAsFactors = FALSE)

# extract year and President for each speech
sona$year <- str_sub(sona$filename, start = 1, end = 4)
sona$President <- str_remove_all(str_extract(sona$filename, "[dA-Z].*\\."), "\\.")

# clean the sona dataset by adding the date and removing unnecessary text
replace_reg <- '(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\n'

sona <-sona %>%
  mutate(speech = str_replace_all(speech, replace_reg , ' ')
         ,date = str_sub(speech, start=1, end=30)
         ,date = str_replace_all(date, "February", "02")
         ,date = str_replace_all(date, "June", "06")
         ,date = str_replace_all(date, "Feb", "02")
         ,date = str_replace_all(date, "May", "05")
         ,date = str_replace_all(date, "Jun", "06")
         ,date = str_replace_all(date, "Thursday, ","")
         ,date = str_replace_all(date, ' ', '-')        
         ,date = str_replace_all(date, "[A-z]",'')
         ,date = str_replace_all(date, '-----', '')
         ,date = str_replace_all(date, '----', '')
         ,date = str_replace_all(date, '---', '')
         ,date = str_replace_all(date, '--', '')
  )

 

sona$date[36] <- "09-02-2023"
sona$year[36] <- "2023"
sona$date <- dmy(sona$date)
sona$mfumu <- as.integer(factor(sona$President))-1

```

```{r MLP Bag of words, include=FALSE}

unnest_reg <- "[^\\w_#@']"
replace_reg <- '(https?:.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;'

tidy_sentences <- sona %>%
  mutate(speech, speech = str_replace_all(speech, "’", "'")) %>%  
  mutate(speech = str_replace_all(speech, replace_reg, '')) %>%            
  unnest_tokens(sentence, speech, token = 'sentences') %>%
  filter(str_detect(sentence, '[a-z]')) %>%
  mutate(Sentence_ID =  row_number())
  

tidy_words<- tidy_sentences %>%
  mutate(sentence, sentence = str_replace_all(sentence, "’", "'")) %>%  
  mutate(sentence = str_replace_all(sentence, replace_reg, '')) %>%            
  unnest_tokens(word, sentence, token = 'words') %>%
  filter(str_detect(word, '[a-z]')) %>% 
  filter(!word %in% stop_words$word) %>% 
  group_by(Sentence_ID, word, President) %>% summarise(n = n()) %>% ungroup()

set.seed(2023)
speech_word_bag<- tidy_words %>%
 count(word) %>%
  top_n(200, wt = n) %>%
  select(-n)

speech_tdf <- tidy_words %>%
  inner_join(speech_word_bag)%>%
 group_by(Sentence_ID,President, word) %>%
  count() %>%
  mutate(total = sum(n)) %>%
  ungroup()

bag_of_words<- speech_tdf %>% 
  select(Sentence_ID, President, word, n) %>% 
  pivot_wider(names_from = word, values_from = n, values_fill = 0, names_repair = "unique")


tf_tdf <- tidy_words %>%
  bind_tf_idf(word, Sentence_ID, n)

tfidf<- tf_tdf %>% 
  select(Sentence_ID,word, tf_idf) %>% 
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0, names_repair = "unique") %>% 
    left_join(tidy_sentences %>% select(Sentence_ID,President))

```

```{r Feed Forward NN, include=FALSE}
#Neural Networks 
bag_of_words$mfumu <- as.integer(factor(bag_of_words$President))-1

set.seed(2023)
sample_size <- floor(0.8 * nrow(bag_of_words))
train_indices <- sample(seq_len(nrow(bag_of_words)), size = sample_size)

train_data <-bag_of_words[train_indices, ]
test_data <- bag_of_words[-train_indices, ]

# Create a bag of words matrix for the training and test data
max_words <- ncol(bag_of_words)
x_train_bag<- as.matrix(train_data[, 3:(max_words-1)]) # Columns 3 and onwards are the BoW columns
x_test_bag <- as.matrix(test_data[, 3:(max_words-1)])
y_train_bag <- to_categorical(train_data$mfumu, 
                              num_classes =length(unique(bag_of_words$mfumu)))
y_test_bag <- to_categorical(test_data$mfumu,
                             num_classes =length(unique(bag_of_words$mfumu)))

# Build a neural network model
#set.seed(2023)
#model <- keras_model_sequential()
#model %>%
#  layer_dense(units = 8, activation = 'relu', input_shape = c(max_words-3)) %>%
#  layer_dropout(rate = 0.5) %>%
#  layer_dense(units = max(train_data$mfumu) + 1, activation = 'softmax')


#model %>% compile(
#  loss = 'categorical_crossentropy',
# optimizer = 'adam',
#  metrics = 'accuracy'
#)

# Train the model
#history <- model %>% fit(
#  x_train_bag, y_train_bag, 
#  epochs = 20, 
#  validation_split = 0.2, shuffle = TRUE
#)
#plot(history)

# Evaluate the model
#NN_results <- model %>%
#  evaluate(x_test_bag, y_test_bag, batch_size=128, verbose = 2)
#save(NN_results, file = "FFNeural.RData")

load("FFNeural.RData")
```

```{r NLP word embeddings, include=FALSE}
training_ids <- bag_of_words %>% 
  group_by(President) %>% 
  slice_sample(prop = 0.8) %>% 
  ungroup() %>%
  select(Sentence_ID)


max_features <- 10000        # choose max_features most popular words
tokenizer = text_tokenizer(num_words = max_features)
fit_text_tokenizer(tokenizer, tidy_sentences$sentence)

sequences = tokenizer$texts_to_sequences(tidy_sentences$sentence)

tidy_sentences$mfumu<- as.integer(factor(tidy_sentences$President))
y <- as.integer(tidy_sentences$mfumu)-1

training_rows <- which(tidy_sentences$Sentence_ID %in%
                         training_ids$Sentence_ID)

train <- list()
train$x <- sequences[training_rows]
train$y <- y[training_rows]
train$y<- to_categorical(train$y,  num_classes = length(unique(tidy_sentences$mfumu)))



test <- list()
test$x <-  sequences[-training_rows]
test$y <-  y[-training_rows]
test$y<- to_categorical(test$y,  num_classes = length(unique(tidy_sentences$mfumu)))

maxlen <- 32               
x_train <- train$x %>% pad_sequences(maxlen = maxlen)
x_test <- test$x %>% pad_sequences(maxlen = maxlen)

##model <- keras_model_sequential() %>% 
#  layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  #layer_dropout(0.2) %>%
 # layer_flatten() %>%
  #layer_dense(32, activation = "relu") %>%
  #layer_dense(units = 6, activation = "softmax")

#model %>% compile(
 # loss = "categorical_crossentropy",
  #optimizer = "adam",
  #metrics = "accuracy"
#)

#history <- model %>%
 # fit(x_train, train$y, epochs = 10, verbose = 0)
#plot(history)

#NN_embedding_results <- model %>% evaluate(x_test, test$y, batch_size = 64, verbose = 2)
  
load("NN_embeded.RData")
  
```

```{r CNN, include=FALSE}
#model <- keras_model_sequential() %>% 
 # layer_embedding(max_features, output_dim = 10, input_length = maxlen) %>%
  #layer_dropout(0.2) %>%
  #layer_conv_1d(filters = 64, kernel_size = 8, activation = "relu") %>%
  #layer_max_pooling_1d(pool_size = 2) %>%
  #layer_flatten() %>%
  #layer_dense(32, activation = "relu") %>%
  #layer_dense(units = 6, activation = "softmax")

#model %>% compile(
 # loss = "categorical_crossentropy",
  #optimizer = "adam",
  #metrics = "accuracy"
#)

#history <- model %>%
 # fit(x_train, train$y,
  #  batch_size = 64, epochs = 10, verbose = 0)
#plot(history)

#CNN_embedding_results <- model %>% evaluate(x_test, test$y, batch_size = 64, verbose = 2)
load("CNN.RData")
```

```{r Random Forest, include=FALSE}
#Bag of words
set.seed(2023)
train_data$mfumu<-as.factor(train_data$mfumu)
test_data$mfumu<- as.factor(test_data$mfumu)

#Define a grid of hyperparameters
#ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
#randomForest_grid <- expand.grid(
#  mtry = c(10, 20))

#randomForest_gridsearch <- train(mfumu ~ ., data = train_data[, -(1:2)], 
#                     method = 'rf', 
 #                     trControl = ctrl, 
  #                     verbose = F, 
   #                   tuneGrid = randomForest_grid)

#rf_pred <- predict(randomForest_gridsearch, newdata = test_data[,-(1:2)]) 
## caret also provides confusion matrix options:
#rf_confusion<- confusionMatrix(rf_pred, test_data$mfumu) #First predicted, then truth
#rf_data<-list(randomForest_grid, rf_pred, rf_confusion)
#save(rf_data,file = "RandomForest_all.RData")

load("RandomForest_all.RData")

```

```{r GBM, include = FALSE}
#set.seed(2023)
#ctrl <- trainControl(method = 'cv', number = 5, verboseIter = T)
#gbm_grid <- expand.grid(n.trees = c(100),
            #            interaction.depth = c(1, 2, 6),
             #           shrinkage = c(0.01, 0.005, 0.001),
              #          n.minobsinnode = 1)


#gbm_gridsearch <- train(mfumu ~ ., data = train_data[, -(1:2)], 
 #                       method = 'gbm', 
  #                    trControl = ctrl, 
   #                    verbose = F, 
    #                   tuneGrid = gbm_grid)

## Prediction
#gbm_pred <- predict(gbm_gridsearch,test_data[,-(1:2)])
#gbm_confusion<- confusionMatrix(gbm_pred, test_data$mfumu)

#gbm_all<- list(gbm_gridsearch,gbm_pred, gbm_confusion)
#save(gbm_all, file = "GradientBoosts_all.RData")
load("GradientBoosts_all.RData")

```

#### Bag of words

BoW is a text representation method that transforms a document or piece of text into a structured format suitable for computational analysis. The core idea behind BoW is to create a "vocabulary" or a set of unique words (or tokens) found in a given corpus of text. Each document in the corpus is then represented as a vector, where the elements of the vector correspond to the frequency or presence of each word in the vocabulary. In this format, the each sentence can be represented in a way that the machine learning models in the classification task for this paper

#### Word Embeddings

Word embeddings are essentially numerical vector representations of words, learned through extensive neural network training on vast text corpora. They map words to points in a continuous vector space, where words with similar meanings or usage patterns are positioned closer together. The key advantage of embeddings is their ability to preserve the semantic relationships between words. Embeddings enable neural networks to understand and process text at a more granular level, as opposed to the BoW approach, which simply counts word occurrences

#### Term Frequency-Inverse Document Frequency (TF-IDF)

TF_IDF is a statistical structuring of data method that evaluates the importance of a term within a document relative to its frequency across a collection of documents. The TF-IDF model operates on the principle that words that occur frequently in one document but less frequently in others are indicative of that document's subject matter. Tf-idf features often give better accuracy in predictive modelling than using word frequencies.

Term Frequency (TF): \begin{equation}
\text{TF}(t, d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d}
\end{equation}

Inverse Document Frequency (IDF): \begin{equation}
\text{IDF}(t, D) = \log\left(\frac{\text{Total number of documents in the corpus } |D|}{\text{Number of documents containing term } t + 1}\right)
\end{equation}

TF-IDF: \begin{equation}
\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
\end{equation}

### Models

The machine learning models used in the classification problem were Feed-Forward Neural Netowrk, Convolutional Neural Network, Random Forest and Gradient Boosts.

#### Feed forward Neural Network

A feedforward neural network, often referred to as a multilayer perceptron (MLP), is a fundamental artificial neural network architecture used for various machine learning and pattern recognition tasks. Each neuron in a layer receives input from neurons in the previous layer, applies a weighted sum of these inputs, and then passes the result through an activation function. The activation functions used in the hidden layers was Rectified Linear Units (ReLU) and in the output layer was the softmax which is appropriate for classification tasks. Training a feedforward neural network involves adjusting the weights assigned to each connection between neurons to minimize the difference between the predicted output and the actual target values using Backpropagation and optimization algorithms.

```{=tex}
\begin{align*}
w_{ij}^{(l)} &\leftarrow w_{ij}^{(l)} - \alpha \frac{\partial C}{\partial w_{ij}^{(l)}} \\
w_{ij}^{(l)} &\text{ is the weight being optimized at layer }l \\
\alpha &\text{ is the learning rate} \\
\frac{\partial C}{\partial w_{ij}^{(l)}} &\text{ is the partial derivative of the Cost function being optimized}
\end{align*}
```
A learning rate,\alpha, of 0.5 was used and Rectified Linear Units (ReLU) activation in the hidden layer. The softmax activation function was used in the output layer.

#### Convolutional Neural Networks (CNNs)

CNNs consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The key innovation in CNNs is the use of convolutional layers, which apply a set of learnable filters (kernels) to input data, extracting spatial features and patterns from the images.Pooling layers reduce the spatial dimensions of the feature maps, effectively down-sampling the data and retaining important information. Fully connected layers process the extracted features to make predictions or classification

The convolution operation applies a filter (kernel) to an input image to produce a feature map. In this equation, I represents the input image, K is the filter (kernel), and denotes the convolution operation.

```{=tex}
\begin{align*}
(I * K)(x, y) = \sum_{i} \sum_{j} I(x - i, y - j) * K(i, j)\\
\text{Max-Pooling}(I)(x, y) = \max_{i, j} I(sx + i, sy + j)
\end{align*}
```
In this paper, the learning rate in the dropout layer was tuned. A ReLU activation function was used in the fully connected hidden layer,while a softmax activation was used in the output layer.

#### Random Forests

Random Forest works by constructing a forest of decision trees, each trained on a random subset of the data and a random subset of the features. This randomness ensures diversity among the individual trees, making the ensemble less prone to overfitting and more resilient to noise in the data.

```{=tex}
\begin{align*}
&\text{Splitting Criteria:} \quad J(D, f) = \sum_{d \in D} \left( p_d \cdot H(y_{d}) - p_{l} \cdot H(y_{l}) - p_{r} \cdot H(y_{r}) \right) \\
&\text{where} \quad H(y) \text{ is the impurity measure for class } y \\
&\text{Decision Tree:} \quad f(X) = \sum_{t=1}^{T} I(x \in R_t) \cdot c_t
\end{align*}
```
In training the model, this paper performed 5 fold cross-validation across a grid of the hyperparameter that controls the number of features randomly selected at each split when growing a tree.

#### Gradient Boosting Machine (GBMs)

GBMs fall under the category of ensemble learning, where multiple weak learners (typically decision trees) are combined to form a strong predictive model. Through an iterative process of models,the algorithm used assigns higher weights to the data points that were misclassified or predicted with high error in the previous model.

```{=tex}
\begin{align*}
L(\theta) = \sum_{i=1}^{n} l(y_i, F(x_i, \theta)) + \Omega(\theta)
\end{align*}
```
Here, $l$ is the loss function, $F(x_i, \theta)$ is the current model's prediction, and $\Omega(\theta)$ represents a regularization term to prevent overfitting.

$$
\theta_{t+1} = \theta_t - \nu \nabla L(\theta_t)\\F(x) = \sum_{t=1}^{T} F_t(x)
$$

The final prediction, $F(x)$, is the summation of predictions from each boosting iteration. In this paper, we perform a grid search over the learning rate,$\nu$,(0.01,0.0005, 0.0001) and the number of trees (100, 200, 500) to be included in each ensemble, $T$. These hyper parameters are tuned and 5-fold cross validation is carried out with each iteration. The hyperparemters that produce the lowest cross validation error is used to fit the final model.

## Results

The training set was used to train the different models and the test set was used to make predictions.

```{r}
#|label: tbl-accuracy
#|ttbl-cap: Accuracy on test data in each Machine Learning models employed

model_accuracies<- round(c(NN_results[[2]], NN_embedding_results[[2]],
                     CNN_embedding_results[[2]],
                     rf_data[[3]]$overall[["Accuracy"]],
                     gbm_all[[3]]$overall[["Accuracy"]]
                     ),4)

model_names<- c("Feed Forward Neural Network (Bag of Words structure)",
                "Feed Forward Neural Network (Embeded Words structure)",
                "Convolutional Neural Nework (Embeded Words structure)",
                "Random Forest",
                "Gradient Boost Machine")

kable(cbind(model_names, model_accuracies), col.names = c("Model", "Test Accuracy"))

```

Table @tbl-accuracy shows the prediction accuracy of each of the models on the test set. It is seen that the standard feed forward neural network with embeded words structure performed the best on the test set having a test accuracy of around 0.5 which is slightly higher than the prediction accuracy of the CNN.

```{r}
#| label: tbl-clssaloc
#| tbl-cap: Class representation of presidents 

pres<- c("De Klerk", "Mandela", "Mbeki", "Mothlane", "Zuma", "Ramaphosa")
clss<- c("Class 0", "Class 1","Class 2","Class 3","Class 4","Class 5")

kable(rbind(clss,pres))
```

```{r}
#| label: tbl-rfmetrics
#| tbl-cap: Random Forest Prediction Performance Metrics

kable(round(rf_data[[3]]$byClass[,c(1,2,5,6,7)],4))

```

```{r}
#| label: tbl-gbmmetrics
#| tbl-cap: Gradient Boosts Prediction Performance Metrics

kable(round(gbm_all[[3]]$byClass[,c(1,2,5,6,7)],4))

```

Table @tbl-clssaloc, Table @tbl-rfmetrics, and Table @tbl-gbmmetrics help us to analyze the performance of the predictions for each of the presidents in when random forests and GMB were implemented. It can be seen by the F1-score in both models perform bes in terms of classifying instances where the sentences belonging and not belonging to President Ramaphosa. Notably, in both Table @tbl-rfmetrics and Table @tbl-gbmmetrics the sensitivity for predicting sentences made by De-Klerk and Mothlane were very low while specficity was close to 1 for both of them in the two methods. This shows that the models performed poorly at classifying sentences that belonged to the two presidents.

## Conclusion

This paper studied the State of the Nation Addresses made by the previous six South African Presidents. Natural Language Processing (NLP) models were used to make predictions to classify sentences according to which president was the source of the sentence. The sentences were structured using the Bag of Words method. Feed Forward Neural networks performed best in terms of test accuracy, having a test accuracy of just around 0.5. Further studies could explore structuring the data as Term Frequency-Inverse Document Frequency format in order to obtain better predictions. This was not done in this paper due to computational limitations. Text mining and NLP can have very useful potential in speech classification,and reveal insights in the world of political communication.

\newpage

### References

::: {#refs}
:::
